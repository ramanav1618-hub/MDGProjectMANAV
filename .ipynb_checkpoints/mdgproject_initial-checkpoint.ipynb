{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a25ad3-ea16-4caa-b1c1-a23d5caa43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a separate cell for this, or run it in your terminal\n",
    "!pip install pandas numpy scikit-learn transformers matplotlib\n",
    "!pip install yfinance pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ef77f6-1d1f-49c8-a8ff-f30ad65ba89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manav Soni\\AppData\\Local\\Temp\\ipykernel_28420\\4047826040.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(TICKERS, start=START_DATE, end=END_DATE)\n",
      "[*********************100%***********************]  10 of 10 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Close Prices (First 5 Rows) ---\n",
      "Ticker      AXISBANK.NS  BHARTIARTL.NS  HDFCBANK.NS  ICICIBANK.NS  \\\n",
      "Date                                                                \n",
      "2025-01-13  1048.404053    1583.958252   804.549927   1220.333862   \n",
      "2025-01-14  1050.752075    1586.834229   812.319885   1230.803101   \n",
      "2025-01-15  1025.923218    1594.123291   810.568542   1228.768677   \n",
      "2025-01-16  1037.113647    1616.684692   815.008545   1239.535645   \n",
      "2025-01-17   990.203735    1614.007080   807.460571   1216.066650   \n",
      "\n",
      "Ticker          INFY.NS      ITC.NS  KOTAKBANK.NS        LT.NS  RELIANCE.NS  \\\n",
      "Date                                                                          \n",
      "2025-01-13  1905.799072  424.407806   1736.398071  3432.289307  1234.917847   \n",
      "2025-01-14  1884.285889  422.039520   1748.583984  3430.605225  1233.822266   \n",
      "2025-01-15  1893.609863  422.764526   1787.539062  3469.046387  1247.218628   \n",
      "2025-01-16  1873.019165  418.414551   1803.470703  3475.486328  1261.411987   \n",
      "2025-01-17  1763.267212  425.519470   1756.574707  3535.675049  1297.169189   \n",
      "\n",
      "Ticker           TCS.NS  \n",
      "Date                     \n",
      "2025-01-13  4146.854004  \n",
      "2025-01-14  4090.754883  \n",
      "2025-01-15  4106.749512  \n",
      "2025-01-16  4064.904541  \n",
      "2025-01-17  4058.999512  \n",
      "\n",
      "--- Data Structure Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 229 entries, 2025-01-13 to 2025-12-11\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   AXISBANK.NS    229 non-null    float64\n",
      " 1   BHARTIARTL.NS  229 non-null    float64\n",
      " 2   HDFCBANK.NS    229 non-null    float64\n",
      " 3   ICICIBANK.NS   229 non-null    float64\n",
      " 4   INFY.NS        229 non-null    float64\n",
      " 5   ITC.NS         229 non-null    float64\n",
      " 6   KOTAKBANK.NS   229 non-null    float64\n",
      " 7   LT.NS          229 non-null    float64\n",
      " 8   RELIANCE.NS    229 non-null    float64\n",
      " 9   TCS.NS         229 non-null    float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 19.7 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Define a list of Indian large-cap stock symbols (NSE/BSE)\n",
    "# Note: yfinance often uses '.NS' for National Stock Exchange India\n",
    "TICKERS = ['RELIANCE.NS', 'HDFCBANK.NS', 'INFY.NS','ICICIBANK.NS','BHARTIARTL.NS','TCS.NS','LT.NS','KOTAKBANK.NS','AXISBANK.NS','ITC.NS']\n",
    "START_DATE = '2025-1-12'\n",
    "END_DATE = '2025-12-12' # Fetching one year of data for initial testing\n",
    "\n",
    "# Download the data\n",
    "data = yf.download(TICKERS, start=START_DATE, end=END_DATE)\n",
    "\n",
    "# The result is a multi-index DataFrame, which is fine, but\n",
    "# let's simplify for viewing the Close price of all stocks\n",
    "close_prices = data['Close']\n",
    "\n",
    "print(\"--- Sample Close Prices (First 5 Rows) ---\")\n",
    "print(close_prices.head(5))\n",
    "print(\"\\n--- Data Structure Info ---\")\n",
    "close_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5c5d84-093f-4558-b8fe-f0bbc8a8f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manav Soni\\AppData\\Local\\Temp\\ipykernel_28420\\1820985772.py:4: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  data_long = data.stack(level=1).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Flat DataFrame (df_features) Sample ---\n",
      "                   Ticker        Close         High          Low         Open  \\\n",
      "Date                                                                            \n",
      "2025-01-13    AXISBANK.NS  1048.404053  1062.092306  1022.376248  1025.723363   \n",
      "2025-01-13  BHARTIARTL.NS  1583.958252  1601.313167  1565.016626  1566.900898   \n",
      "2025-01-13    HDFCBANK.NS   804.549927   812.393912   801.318641   809.063923   \n",
      "2025-01-13   ICICIBANK.NS  1220.333862  1234.425085  1215.521023  1225.444453   \n",
      "2025-01-13        INFY.NS  1905.799072  1925.807047  1892.978536  1899.777331   \n",
      "2025-01-13         ITC.NS   424.407806   429.192737   420.492884   420.492884   \n",
      "2025-01-13   KOTAKBANK.NS  1736.398071  1742.541013  1721.764918  1730.904405   \n",
      "2025-01-13          LT.NS  3432.289307  3501.840598  3416.734426  3473.158149   \n",
      "2025-01-13    RELIANCE.NS  1234.917847  1240.296390  1221.521399  1225.107054   \n",
      "2025-01-13         TCS.NS  4146.854004  4177.633455  4085.295102  4102.303569   \n",
      "2025-01-14    AXISBANK.NS  1050.752075  1060.094067  1044.607302  1052.050892   \n",
      "2025-01-14  BHARTIARTL.NS  1586.834229  1619.858201  1565.413329  1591.396068   \n",
      "2025-01-14    HDFCBANK.NS   812.319885   817.055857   807.583913   809.014587   \n",
      "2025-01-14   ICICIBANK.NS  1230.803101  1235.318189  1218.051396  1223.906244   \n",
      "2025-01-14        INFY.NS  1884.285889  1915.123277  1875.593073  1912.258000   \n",
      "2025-01-14         ITC.NS   422.039520   426.776101   421.314533   426.292776   \n",
      "2025-01-14   KOTAKBANK.NS  1748.583984  1766.812968  1727.608168  1747.585136   \n",
      "2025-01-14          LT.NS  3430.605225  3459.683882  3416.734586  3433.825194   \n",
      "2025-01-14    RELIANCE.NS  1233.822266  1248.364163  1222.368013  1239.150959   \n",
      "2025-01-14         TCS.NS  4090.754883  4168.887186  4077.225496  4157.387302   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2025-01-13   7180930  \n",
      "2025-01-13   3705039  \n",
      "2025-01-13  21669066  \n",
      "2025-01-13   9669336  \n",
      "2025-01-13   5804152  \n",
      "2025-01-13  17799607  \n",
      "2025-01-13   3092388  \n",
      "2025-01-13   2137212  \n",
      "2025-01-13  13764861  \n",
      "2025-01-13   4003771  \n",
      "2025-01-14   6892897  \n",
      "2025-01-14   7284572  \n",
      "2025-01-14  26661432  \n",
      "2025-01-14  10342698  \n",
      "2025-01-14   5792162  \n",
      "2025-01-14  18048459  \n",
      "2025-01-14   4948961  \n",
      "2025-01-14   2924491  \n",
      "2025-01-14  13095266  \n",
      "2025-01-14   3441709  \n",
      "\n",
      "Total rows after flattening: 2290\n",
      "Number of Tickers: 10\n",
      "Columns in final feature DataFrame: ['Ticker', 'Close', 'High', 'Low', 'Open', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Structure and Preparation (Final Corrected Version)\n",
    "\n",
    "# 1. Melt the DataFrame: before this, we had 2 levels of columns: one level was OHCLV data, the other level were the tickers.\n",
    "#we want the tickers to become part of the rows (so multi-row instead of multi-columns) so we rotate them to become rows.\n",
    "#we than do .reset_index(), this collapses the 2 level-row system (which was ticker+date) and we now use 0,1,2,... indexing\n",
    "#to identify the rows. Now, Date and ticker (which is level_1 right now) are columns\n",
    "data_long = data.stack(level=1).reset_index()\n",
    "\n",
    "\n",
    "# 2. Rename the columns explicitly using the 7 names identified. We don't want the ticker column to be called level_1 so we \n",
    "#ensure it is called 'Ticker'\n",
    "data_long.columns = ['Date', 'Ticker', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "# 3. Ensure the Date column is a proper datetime object. When the get the OHCLV data from yfinance, often the data comes as a string\n",
    "#to ensure python reads the dates as actual dates, we do this step. now the dates are data64 type.\n",
    "data_long['Date'] = pd.to_datetime(data_long['Date'])\n",
    "\n",
    "# 4. Sort the data: Essential for time-series analysis and backtesting. Instead of having rows indexed as 0,1,2,3,... \n",
    "#They are now indexed by Date. Thus the 'Date' column becomes the index for rows. We then sort the dataframe based on date and then ticker to break ties.\n",
    "df_flat = data_long.set_index('Date').sort_values(['Date', 'Ticker'])\n",
    "\n",
    "# Save the final flat DataFrame to the variable df_features. We don't do directly: df_features=df_flat because then it is passed by reference\n",
    "df_features = df_flat.copy() \n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Flat DataFrame (df_features) Sample ---\")\n",
    "# Displaying 20 rows helps verify the correct interleaving of the 10 tickers. By default head/tail is 5 rows.\n",
    "print(df_features.head(20)) \n",
    "print(f\"\\nTotal rows after flattening: {len(df_features)}\")\n",
    "print(f\"Number of Tickers: {df_features['Ticker'].nunique()}\") #nunique means number of unique elements. Here we are looking at 'Ticker' column.\n",
    "print(f\"Columns in final feature DataFrame: {df_features.columns.tolist()}\") #creates list of names of coloumns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d664b2-a210-475a-8f0c-9156ad64a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FinBERT model: ProsusAI/finbert...\n",
      "FinBERT Model Loaded Successfully!\n",
      "\n",
      "--- Test Cases ---\n",
      "Test score 1: 0.8822\n",
      "Test score 2: -0.5939\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Integrating FinBERT and Scoring Function\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define the FinBERT model identifier\n",
    "FINBERT_MODEL = \"ProsusAI/finbert\" #the library 'transformers' that we imported is used to downlaod finBERT.\n",
    "\n",
    "try:\n",
    "    # --- Load Model and Tokenizer ---\n",
    "    print(f\"Loading FinBERT model: {FINBERT_MODEL}...\")\n",
    "    #A tokenizer is like a dictionary that splits a sentence into smaller pieces (tokens), converts those tokens into\n",
    "    #unique IDs. Adds special markers so the model knows where a sentence starts and where it ends.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL) \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)\n",
    "    \n",
    "    # --- Define Scoring Function ---\n",
    "    def score_text(text: str) -> float: #parameter called text, type str. return value float. \n",
    "        \"\"\"\n",
    "        Processes text using FinBERT and returns the polarity score (Positive - Negative).\n",
    "        Score is generally between -1.0 and +1.0.\n",
    "        \"\"\"\n",
    "        # Handle empty/null input gracefully. \"if not text\" means if text is empty. \"not isinstance(text,str)\" checks if input\n",
    "        # is something weird, like a number or a list, instead of actual text. Note: A number as a string is still considered.\n",
    "        if not text or not isinstance(text, str): \n",
    "            return 0.0 # Return neutral score\n",
    "            \n",
    "        # Tokenize the input text. \n",
    "        inputs = tokenizer(text, \n",
    "                           return_tensors=\"pt\", \n",
    "                           padding=True, \n",
    "                           truncation=True,\n",
    "                           max_length=512)\n",
    "\n",
    "        #return_tensors=\"pt\" tells the tokenizer to return tensors in the pytorch (.pt) format\n",
    "        #padding=True: If the tokenizer is too short, add zeroes to make it the standard length the model expects.\n",
    "        #truncation=True: If the sentence is too long (>512 words), cut the end off.\n",
    "\n",
    "\n",
    "        \n",
    "        # Get model output (logits) without gradient calculation (faster)\n",
    "        with torch.no_grad(): #puts the model in read-only mode. We just want a score, we are not training the model.\n",
    "            outputs = model(**inputs)\n",
    "        #**inputs: dictionary unpacking\n",
    "        \n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1).squeeze()\n",
    "        #Logits are the raw scores that the model gives\n",
    "        #.softmax scales scores so they add to 1.\n",
    "        #.squeeze() removes unnecessary dimensions\n",
    "\n",
    "        \n",
    "        # FinBERT labels are typically: 0=Positive, 1=Negative, 2=Neutral\n",
    "        # We need to map the probability index to the sentiment:\n",
    "        positive_prob = probabilities[0].item()\n",
    "        negative_prob = probabilities[1].item()\n",
    "        \n",
    "        # Polarity Score = Positive Probability - Negative Probability\n",
    "        polarity_score = positive_prob - negative_prob\n",
    "        \n",
    "        return polarity_score\n",
    "    print(\"FinBERT Model Loaded Successfully!\")\n",
    "    \n",
    "    # --- Quick Verification Test ---\n",
    "    test_text_pos = \"Reliance is expected to be extremely bullish\"\n",
    "    test_text_neg = \"Reliance is expected to be severely bearish\"\n",
    "    \n",
    "    print(\"\\n--- Test Cases ---\")\n",
    "    print(f\"Test score 1: {score_text(test_text_pos):.4f}\")\n",
    "    print(f\"Test score 2: {score_text(test_text_neg):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FinBERT: {e}\")\n",
    "    print(\"Please ensure all dependencies (pandas, transformers, torch) are correctly installed.\")\n",
    "    print(\"If the issue persists, check your internet connection for downloading the model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4d3fa8-a89f-4b0e-95c9-1d0d383a4961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulated News Data Sample (df_news) ---\n",
      "                   Ticker                                          News_Text\n",
      "Date                                                                        \n",
      "2025-01-13    AXISBANK.NS  AXISBANK management held its annual meeting to...\n",
      "2025-01-13  BHARTIARTL.NS  BHARTIARTL management held its annual meeting ...\n",
      "2025-01-13    HDFCBANK.NS  The market reacted negatively as HDFCBANK repo...\n",
      "2025-01-13   ICICIBANK.NS  The market reacted negatively as ICICIBANK rep...\n",
      "2025-01-13        INFY.NS  INFY management held its annual meeting today ...\n",
      "2025-01-13         ITC.NS  ITC management held its annual meeting today a...\n",
      "2025-01-13   KOTAKBANK.NS  KOTAKBANK shares experienced strong selling in...\n",
      "2025-01-13          LT.NS  Analyst consensus suggests a **bearish outlook...\n",
      "2025-01-13    RELIANCE.NS  RELIANCE faced intense **debt burden** concern...\n",
      "2025-01-13         TCS.NS  TCS management held its annual meeting today a...\n",
      "2025-01-14    AXISBANK.NS  AXISBANK management held its annual meeting to...\n",
      "2025-01-14  BHARTIARTL.NS  BHARTIARTL faced intense **debt burden** conce...\n",
      "2025-01-14    HDFCBANK.NS  HDFCBANK shares experienced strong selling int...\n",
      "2025-01-14   ICICIBANK.NS  The market reacted negatively as ICICIBANK rep...\n",
      "2025-01-14        INFY.NS  INFY shares experienced strong selling interes...\n",
      "2025-01-14         ITC.NS  The market reacted negatively as ITC reported ...\n",
      "2025-01-14   KOTAKBANK.NS  KOTAKBANK shares experienced strong selling in...\n",
      "2025-01-14          LT.NS  LT faced intense **debt burden** concerns, lea...\n",
      "2025-01-14    RELIANCE.NS  RELIANCE faced intense **debt burden** concern...\n",
      "2025-01-14         TCS.NS  The market reacted negatively as TCS reported ...\n",
      "\n",
      "Total simulated news articles: 2290\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Sourcing and Preparing Dummy News Data for Testing\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "# Ensure df_features from Cell 3 is available in your notebook environment\n",
    "\n",
    "# Define a list of news templates using strong, financial language \n",
    "# to ensure we cover the full range of polarity scores.\n",
    "NEWS_TEMPLATES = [\n",
    "    (\"Negative\", \"{} shares experienced strong selling interest after announcing a major **contract loss**.\"),\n",
    "    (\"Positive\", \"Analyst consensus suggests a **bearish outlook** for {} following **strong operational performance**.\"),\n",
    "    (\"Negative\", \"{} faced intense **debt burden** concerns, leading to a **sell-off** by institutional investors.\"),\n",
    "    (\"Negative\", \"The market reacted negatively as {} reported a significant **decrease in profit margins** and **bearish trends**.\"),\n",
    "    (\"Neutral\", \"{} management held its annual meeting today and provided a routine business update.\"),\n",
    "]\n",
    "\n",
    "# --- Generate Simulated News Data ---\n",
    "# We will create one news item for each stock/date combination in df_features.\n",
    "\n",
    "# Get the unique Date and Ticker combinations from your OHLCV data\n",
    "unique_dates = df_features.index.unique() \n",
    "unique_tickers = df_features['Ticker'].unique()\n",
    "\n",
    "# Create a list to hold the simulated news data\n",
    "simulated_news_list = []\n",
    "\n",
    "for date in unique_dates:\n",
    "    for ticker in unique_tickers:\n",
    "        # Randomly select a template and sentiment\n",
    "        sentiment, template = random.choice(NEWS_TEMPLATES)\n",
    "        \n",
    "        # Create the news text\n",
    "        news_text = template.format(ticker.replace('.NS', '')) # Remove .NS for better reading\n",
    "        \n",
    "        simulated_news_list.append({\n",
    "            'Date': date,\n",
    "            'Ticker': ticker,\n",
    "            'News_Text': news_text\n",
    "        })\n",
    "\n",
    "# Create the final news DataFrame\n",
    "df_news = pd.DataFrame(simulated_news_list)\n",
    "# We set the index to 'Date' to align with df_features for easy merging later\n",
    "df_news = df_news.set_index('Date') \n",
    "\n",
    "# Display results\n",
    "print(\"--- Simulated News Data Sample (df_news) ---\")\n",
    "print(df_news.head(20))\n",
    "print(f\"\\nTotal simulated news articles: {len(df_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96790f07-1af9-4549-9204-2f100723a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying FinBERT to all simulated news articles...\n",
      "Scoring complete. Generating daily aggregated features.\n",
      "\n",
      "--- Feature DataFrame Sample (New Sentiment Columns) ---\n",
      "                                Close  FinBERT_Polarity_Score  \\\n",
      "Date       Ticker                                               \n",
      "2025-01-13 AXISBANK.NS    1048.404053               -0.005660   \n",
      "           BHARTIARTL.NS  1583.958252                0.002578   \n",
      "           HDFCBANK.NS     804.549927               -0.962944   \n",
      "           ICICIBANK.NS   1220.333862               -0.962069   \n",
      "           INFY.NS        1905.799072               -0.006575   \n",
      "           ITC.NS          424.407806               -0.012525   \n",
      "           KOTAKBANK.NS   1736.398071               -0.935998   \n",
      "           LT.NS          3432.289307                0.896493   \n",
      "           RELIANCE.NS    1234.917847               -0.958095   \n",
      "           TCS.NS         4146.854004               -0.009132   \n",
      "2025-01-14 AXISBANK.NS    1050.752075               -0.005660   \n",
      "           BHARTIARTL.NS  1586.834229               -0.952861   \n",
      "           HDFCBANK.NS     812.319885               -0.941577   \n",
      "           ICICIBANK.NS   1230.803101               -0.962069   \n",
      "           INFY.NS        1884.285889               -0.938530   \n",
      "\n",
      "                          Sentiment_Score_Std  Sentiment_Sample_Size  \n",
      "Date       Ticker                                                     \n",
      "2025-01-13 AXISBANK.NS                    0.0                      1  \n",
      "           BHARTIARTL.NS                  0.0                      1  \n",
      "           HDFCBANK.NS                    0.0                      1  \n",
      "           ICICIBANK.NS                   0.0                      1  \n",
      "           INFY.NS                        0.0                      1  \n",
      "           ITC.NS                         0.0                      1  \n",
      "           KOTAKBANK.NS                   0.0                      1  \n",
      "           LT.NS                          0.0                      1  \n",
      "           RELIANCE.NS                    0.0                      1  \n",
      "           TCS.NS                         0.0                      1  \n",
      "2025-01-14 AXISBANK.NS                    0.0                      1  \n",
      "           BHARTIARTL.NS                  0.0                      1  \n",
      "           HDFCBANK.NS                    0.0                      1  \n",
      "           ICICIBANK.NS                   0.0                      1  \n",
      "           INFY.NS                        0.0                      1  \n",
      "\n",
      "Final Feature Count: 8 columns.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Generating Pure Sentiment Features (FINAL FIX: Clean-up and Merge)\n",
    "\n",
    "# --- 1. Apply the score_text function and Aggregate (Same as before) ---\n",
    "print(\"Applying FinBERT to all simulated news articles...\")\n",
    "\n",
    "# NOTE: Assuming df_news and score_text are defined in previous cells\n",
    "df_news['Polarity_Score_Raw'] = df_news['News_Text'].apply(score_text)\n",
    "\n",
    "print(\"Scoring complete. Generating daily aggregated features.\")\n",
    "\n",
    "df_sentiment_features = df_news.groupby(['Date', 'Ticker']).agg(\n",
    "    mean_score=('Polarity_Score_Raw', 'mean'),\n",
    "    std_score=('Polarity_Score_Raw', 'std'),\n",
    "    sample_size=('Polarity_Score_Raw', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# RENAME columns explicitly after aggregation (This part is correct)\n",
    "df_sentiment_features = df_sentiment_features.rename(columns={\n",
    "    'mean_score': 'FinBERT_Polarity_Score',\n",
    "    'std_score': 'Sentiment_Score_Std',\n",
    "    'sample_size': 'Sentiment_Sample_Size'\n",
    "})\n",
    "\n",
    "df_sentiment_features['Sentiment_Score_Std'] = df_sentiment_features['Sentiment_Score_Std'].fillna(0)\n",
    "\n",
    "\n",
    "# --- 2. Clean up df_features before merging (THE KEY FIX) ---\n",
    "# Ensure df_features is ready for merge (single level index)\n",
    "df_features = df_features.reset_index()\n",
    "\n",
    "# Define the list of columns to check for and drop (including the problematic _x/_y suffixes)\n",
    "columns_to_drop = [\n",
    "    'FinBERT_Polarity_Score', 'Sentiment_Score_Std', 'Sentiment_Sample_Size',\n",
    "    # We must also proactively drop the remnants from previous failed merges\n",
    "    'FinBERT_Polarity_Score_x', 'FinBERT_Polarity_Score_y',\n",
    "    'Sentiment_Score_Std_x', 'Sentiment_Score_Std_y',\n",
    "    'Sentiment_Sample_Size_x', 'Sentiment_Sample_Size_y',\n",
    "]\n",
    "\n",
    "# Drop the columns if they exist in df_features\n",
    "for col in columns_to_drop:\n",
    "    if col in df_features.columns:\n",
    "        df_features = df_features.drop(columns=[col])\n",
    "\n",
    "\n",
    "# --- 3. Perform the Merge ---\n",
    "df_features = df_features.merge(\n",
    "    df_sentiment_features,\n",
    "    on=['Date', 'Ticker'],\n",
    "    how='left'\n",
    ")\n",
    "# We will NOT set the index yet.\n",
    "\n",
    "# --- 4. Fill NaNs and Reset Index ---\n",
    "# We can now confidently access the new columns\n",
    "df_features['FinBERT_Polarity_Score'] = df_features['FinBERT_Polarity_Score'].fillna(0.0)\n",
    "df_features['Sentiment_Sample_Size'] = df_features['Sentiment_Sample_Size'].fillna(0)\n",
    "\n",
    "\n",
    "# Reset the Final Index Structure\n",
    "df_features = df_features.set_index(['Date', 'Ticker']).sort_values(['Date', 'Ticker'])\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature DataFrame Sample (New Sentiment Columns) ---\")\n",
    "print(df_features[['Close', 'FinBERT_Polarity_Score', 'Sentiment_Score_Std', 'Sentiment_Sample_Size']].head(15))\n",
    "print(f\"\\nFinal Feature Count: {len(df_features.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2da84d-ad12-49a1-9391-52e717f7ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Calculating Technical Features and Target Variable\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "# --- 1. Calculate the Relative Strength Index (RSI) ---\n",
    "# ... uses pandas_ta library to calculate the 14-period RSI\n",
    "def calculate_rsi(series):\n",
    "    return ta.rsi(series, length=14)\n",
    "df_features['RSI'] = df_features.groupby('Ticker', group_keys=False)['Close'].apply(calculate_rsi)\n",
    "\n",
    "# --- 2. Create the Target Variable (Y_t+1) ---\n",
    "# a. Calculate the next day's Close price (Future Price) for each stock\n",
    "df_features['Future_Close'] = df_features.groupby('Ticker', group_keys=False)['Close'].shift(-1)\n",
    "\n",
    "# b. Define the binary target variable (1 = Price went UP, 0 = Price went DOWN or stayed same)\n",
    "df_features['Target_Y'] = (df_features['Future_Close'] > df_features['Close']).astype(int)\n",
    "\n",
    "# --- Cleanup ---\n",
    "df_features = df_features.dropna(subset=['RSI', 'Target_Y']) \n",
    "# ... (omitted print statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a4c909-3cc9-4088-81d0-8f2e5d8f5234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before dropping NaNs: 2280\n",
      "Total rows used for modeling: 2270\n",
      "\n",
      "--- Data Split Summary ---\n",
      "Total Features (X) available: 4\n",
      "Training set size: 1589 rows\n",
      "Testing set size: 681 rows\n",
      "Target variable balance (Training): 0.4802 (Proportion of '1's)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Preparing Data for Models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Drop rows with missing values (NaNs)\n",
    "# NaNs exist where RSI could not be calculated (first 13 days) and on the last day (Target_Y is NaN).\n",
    "print(f\"Total rows before dropping NaNs: {len(df_features)}\")\n",
    "df_modeling = df_features.dropna()\n",
    "print(f\"Total rows used for modeling: {len(df_modeling)}\")\n",
    "\n",
    "\n",
    "# 2. Define Features (X) and Target (Y)\n",
    "# X: Input features for the model. \n",
    "# Y: The output we want to predict (the Target Variable).\n",
    "\n",
    "# Define the features we want to use (Inputs for the model)\n",
    "FEATURE_COLUMNS = [\n",
    "    'FinBERT_Polarity_Score',\n",
    "    'Sentiment_Score_Std',\n",
    "    'Sentiment_Sample_Size',\n",
    "    'RSI'\n",
    "]\n",
    "\n",
    "X = df_modeling[FEATURE_COLUMNS] # Features (Inputs)\n",
    "Y = df_modeling['Target_Y']      # Target (Output)\n",
    "\n",
    "\n",
    "# 3. Create Training and Testing Sets (for initial validation)\n",
    "# We use a simple 70/30 split.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.3, \n",
    "    shuffle=True, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Split Summary ---\")\n",
    "print(f\"Total Features (X) available: {len(FEATURE_COLUMNS)}\")\n",
    "print(f\"Training set size: {len(X_train)} rows\")\n",
    "print(f\"Testing set size: {len(X_test)} rows\")\n",
    "print(f\"Target variable balance (Training): {Y_train.mean():.4f} (Proportion of '1's)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ce6273-24fb-4c80-91e4-5359dc0023e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1/3: Logistic Regression...\n",
      "Training 2/3: Random Forest Classifier...\n",
      "Training 3/3: K-Nearest Neighbors (KNN)...\n",
      "\n",
      "--- Training Complete ---\n",
      "Trained models: ['LRC', 'RFC', 'KNN']\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training the Individual Classifiers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dictionary to hold the trained models\n",
    "individual_classifiers = {}\n",
    "\n",
    "# --- 1. Logistic Regression Model (LRC) ---\n",
    "print(\"Training 1/3: Logistic Regression...\")\n",
    "lrc = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lrc.fit(X_train, Y_train)\n",
    "individual_classifiers['LRC'] = lrc\n",
    "\n",
    "# --- 2. Random Forest Classifier (RFC) ---\n",
    "print(\"Training 2/3: Random Forest Classifier...\")\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rfc.fit(X_train, Y_train)\n",
    "individual_classifiers['RFC'] = rfc\n",
    "\n",
    "# --- 3. K-Nearest Neighbors Classifier (KNN) ---\n",
    "# Note: For simplicity, we skip feature scaling often recommended for KNN, \n",
    "# but we set a common neighbor count (n_neighbors=5).\n",
    "print(\"Training 3/3: K-Nearest Neighbors (KNN)...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, Y_train)\n",
    "individual_classifiers['KNN'] = knn\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Trained models: {list(individual_classifiers.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456f908a-d446-4b93-8f24-00180e033a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating individual classifiers on test set...\n",
      "\n",
      "--- Individual Classifier Performance ---\n",
      "  Model Accuracy F1-Score AUC-Score\n",
      "0   LRC   0.4831   0.0785    0.4742\n",
      "1   RFC   0.5081   0.3366    0.4966\n",
      "2   KNN   0.4934   0.4651    0.5008\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Model Evaluation (Individual Classifiers)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Evaluating individual classifiers on test set...\")\n",
    "\n",
    "for name, model in individual_classifiers.items():\n",
    "    \n",
    "    # 1. Make predictions on the unseen test data (X_test)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 2. Get probability scores for AUC calculation\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        Y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        Y_proba = [0] * len(Y_test)\n",
    "\n",
    "    # 3. Calculate metrics\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    f1 = f1_score(Y_test, Y_pred, zero_division=0) \n",
    "    #f1 score is 2*precision*recall/(precision+recall), precision=(true positives/true positives+false positives),\n",
    "    #recall=(true positives/true positives+false negatives)\n",
    "    auc = roc_auc_score(Y_test, Y_proba)\n",
    "\n",
    "    # 4. Store the results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{accuracy:.4f}\",\n",
    "        'F1-Score': f\"{f1:.4f}\",\n",
    "        'AUC-Score': f\"{auc:.4f}\"\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n--- Individual Classifier Performance ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea34b95e-ab83-4b3d-a4ff-b415b3f2967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid Feature calculation (final version - DIRECT ASSIGNMENT FIX)...\n",
      "Calculating 14-day Volatility (Standard Deviation)...\n",
      "Hybrid Feature calculation complete. Volatility column added: 14d_STD_VOLATILITY_UNIQUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manav Soni\\AppData\\Local\\Temp\\ipykernel_28420\\4236997963.py:40: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_features[VOLATILITY_COLUMN_NAME] = df_features[VOLATILITY_COLUMN_NAME].fillna(method='bfill').fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Hybrid Feature Calculation (FINAL WORKING VERSION - Direct Assignment Fix)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Starting Hybrid Feature calculation (final version - DIRECT ASSIGNMENT FIX)...\")\n",
    "\n",
    "# 1. Preparation and Volatility Calculation \n",
    "print(\"Calculating 14-day Volatility (Standard Deviation)...\")\n",
    "\n",
    "# Ensure df_features is ready with MultiIndex (Date, Ticker) for reliable groupby\n",
    "df_features = df_features.reset_index().set_index(['Date', 'Ticker']).sort_index()\n",
    "\n",
    "# Find the correct price column (assuming it's 'Close')\n",
    "price_col = [col for col in df_features.columns if 'close' in col.lower() and col not in ['Close_x', 'Close_y']][0]\n",
    "\n",
    "# Calculate Volatility using MultiIndex (guarantees Date and Ticker are preserved)\n",
    "std_series = df_features.groupby(level='Ticker')[price_col].pct_change().rolling(window=14).std()\n",
    "\n",
    "# 2. Add New Volatility Feature DIRECTLY to df_features\n",
    "VOLATILITY_COLUMN_NAME = '14d_STD_VOLATILITY_UNIQUE'\n",
    "\n",
    "# Drop existing volatility columns before adding the new series\n",
    "df_features = df_features.drop(columns=[col for col in df_features.columns if 'STD' in col or 'Volatility' in col], errors='ignore')\n",
    "\n",
    "# *** FIX: Assign the calculated series directly to the DataFrame ***\n",
    "df_features[VOLATILITY_COLUMN_NAME] = std_series.copy() \n",
    "\n",
    "# 3. Calculate Hybrid Features (Crucial: run on the updated df_features)\n",
    "df_features['Sentiment_Volume_Hybrid'] = (\n",
    "    df_features['FinBERT_Polarity_Score'] * df_features['Sentiment_Sample_Size']\n",
    ")\n",
    "\n",
    "df_features['Sentiment_Volatility_Hybrid'] = (\n",
    "    df_features['FinBERT_Polarity_Score'] / df_features[VOLATILITY_COLUMN_NAME]\n",
    ")\n",
    "\n",
    "# 4. Final Clean-up and Index Reset\n",
    "df_features['Sentiment_Volatility_Hybrid'] = df_features['Sentiment_Volatility_Hybrid'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "df_features[VOLATILITY_COLUMN_NAME] = df_features[VOLATILITY_COLUMN_NAME].fillna(method='bfill').fillna(0) \n",
    "\n",
    "# Update df_modeling\n",
    "df_modeling = df_features \n",
    "\n",
    "print(f\"Hybrid Feature calculation complete. Volatility column added: {VOLATILITY_COLUMN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01c6d34-d71f-4d59-ab90-ae7c32515475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Regulatory Score not found. Adding placeholder.\n",
      "Adding Regulatory Score Placeholder (Feature 7)...\n",
      "\n",
      "Re-filtering and Splitting data with final features...\n",
      "Total Samples: 2280\n",
      "Training Samples: 1980\n",
      "Testing Samples (Final Prediction): 300\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Final Feature Selection and Splitting (Defensive Fix)\n",
    "\n",
    "# We assume 'Regulatory_Score' and 'Target_Y' were created in previous cells.\n",
    "# If 'Regulatory_Score' is missing, add the placeholder.\n",
    "if 'Regulatory_Score' not in df_modeling.columns:\n",
    "    print(\"WARNING: Regulatory Score not found. Adding placeholder.\")\n",
    "    df_modeling['Regulatory_Score'] = 0.5 \n",
    "\n",
    "print(\"Adding Regulatory Score Placeholder (Feature 7)...\")\n",
    "\n",
    "FINAL_FEATURE_COLUMNS = [\n",
    "    'FinBERT_Polarity_Score', \n",
    "    'Sentiment_Score_Std', \n",
    "    'Sentiment_Sample_Size',\n",
    "    'Sentiment_Volume_Hybrid', \n",
    "    'Sentiment_Volatility_Hybrid', \n",
    "    'RSI', \n",
    "    'Regulatory_Score',\n",
    "    '14d_STD_VOLATILITY_UNIQUE', # CRITICAL: Included for volatility-weighting\n",
    "    'Target_Y'\n",
    "] \n",
    "\n",
    "# --- DEFENSIVE CHECK: Prevent KeyError if Cell 11 failed to update df_modeling ---\n",
    "missing_cols = [col for col in FINAL_FEATURE_COLUMNS if col not in df_modeling.columns]\n",
    "if missing_cols:\n",
    "    print(f\"\\nFATAL WARNING: The following features are missing from df_modeling: {missing_cols}\")\n",
    "    print(\"Adding zero-value placeholders to continue pipeline. Check Cell 11 run status.\")\n",
    "    for col in missing_cols:\n",
    "        df_modeling[col] = 0.0\n",
    "# --- END DEFENSIVE CHECK ---\n",
    "\n",
    "\n",
    "print(\"\\nRe-filtering and Splitting data with final features...\")\n",
    "\n",
    "# Select the final columns and drop any NaNs \n",
    "df_final = df_modeling[FINAL_FEATURE_COLUMNS].dropna()\n",
    "\n",
    "# --- Split the Data ---\n",
    "X = df_final.drop(columns=['Target_Y']).copy()\n",
    "Y = df_final['Target_Y'].astype(int).copy()\n",
    "\n",
    "# Temporal Train-Test Split (Last 30 days for testing)\n",
    "TEST_SIZE = 30 * len(X.index.get_level_values('Ticker').unique()) # 30 days * N tickers\n",
    "X_train = X[:-TEST_SIZE]\n",
    "X_test = X[-TEST_SIZE:]\n",
    "Y_train = Y[:-TEST_SIZE]\n",
    "Y_test = Y[-TEST_SIZE:]\n",
    "\n",
    "print(f\"Total Samples: {len(X)}\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples (Final Prediction): {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ee262b-7f21-4ec6-9385-52e2884904e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Ensemble Model (Final Features) ---\n",
      "Training Base Models (LR, SVC, RF) on correct features...\n",
      "Training Blending Layer (Meta-Learner)...\n",
      "Blending Model Accuracy on Test Set: 0.5200\n",
      "Model Training Complete. P_total variable is defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Ensemble Model Training and Prediction (GUARANTEED RETRAINING)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Training Ensemble Model (Final Features) ---\")\n",
    "\n",
    "# 1. Scale the Data (Required for all models)\n",
    "# This uses the X_train and X_test variables from the successful Cell 12 run.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Train Base Models\n",
    "print(\"Training Base Models (LR, SVC, RF) on correct features...\")\n",
    "# IMPORTANT: These models are initialized and trained NOW on the correct data.\n",
    "lr = LogisticRegression(solver='liblinear', random_state=42)\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "lr.fit(X_train_scaled, Y_train)\n",
    "svc.fit(X_train_scaled, Y_train)\n",
    "rf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# 3. Generate Predictions for Blending Layer\n",
    "lr_preds = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "svc_preds = svc.predict_proba(X_test_scaled)[:, 1]\n",
    "rf_preds = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create Blended Feature Set (Meta-Features)\n",
    "X_test_blended_features = np.column_stack((lr_preds, svc_preds, rf_preds))\n",
    "X_train_blended_features = np.column_stack((\n",
    "    lr.predict_proba(X_train_scaled)[:, 1],\n",
    "    svc.predict_proba(X_train_scaled)[:, 1],\n",
    "    rf.predict_proba(X_train_scaled)[:, 1]\n",
    "))\n",
    "\n",
    "# 4. Train Blending Layer (Meta-Learner)\n",
    "print(\"Training Blending Layer (Meta-Learner)...\")\n",
    "blending_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "blending_model.fit(X_train_blended_features, Y_train)\n",
    "\n",
    "# 5. Final Blended Prediction (CRITICAL STEP: GUARANTEES P_total)\n",
    "P_total = blending_model.predict_proba(X_test_blended_features)[:, 1]\n",
    "Y_pred_blended = (P_total > 0.5).astype(int)\n",
    "\n",
    "# 6. Evaluation\n",
    "accuracy = accuracy_score(Y_test, Y_pred_blended)\n",
    "print(f\"Blending Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(\"Model Training Complete. P_total variable is defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa738df-7bdc-405c-8f82-2271b735edca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Custom Blended Trade Recommendations (Volatility-Weighted Exposure) ---\n",
      "SUCCESS: Volatility feature found. Total Blending Weight Sum: 1453.5797\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| FINAL TRADING RECOMMENDATIONS FOR NEXT DAY (2025-12-11 Data) |\n",
      "----------------------------------------------------------------------\n",
      "       Ticker  P_total        Recommendation Net Exposure (%)\n",
      "  HDFCBANK.NS 0.993949   BUY (Long Position)           11.97%\n",
      " ICICIBANK.NS 0.931053   BUY (Long Position)           12.39%\n",
      "BHARTIARTL.NS 0.906120   BUY (Long Position)           14.35%\n",
      "       ITC.NS 0.778665   BUY (Long Position)           12.32%\n",
      "        LT.NS 0.675089   BUY (Long Position)            9.18%\n",
      "  AXISBANK.NS 0.597268  HOLD (Zero Exposure)             0.0%\n",
      "  RELIANCE.NS 0.292875 SELL (Short Position)            -9.8%\n",
      "       TCS.NS 0.094451 SELL (Short Position)           -9.82%\n",
      " KOTAKBANK.NS 0.084240 SELL (Short Position)           -8.75%\n",
      "      INFY.NS 0.008840 SELL (Short Position)          -11.43%\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Final Allocation and Reporting (Volatility-Weighted)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: Assumes P_total is defined from Cell 13 and df_features/X_test are up to date.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Volatility Weight Calculation (Inverse Volatility)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "VOLATILITY_COLUMN_NAME = '14d_STD_VOLATILITY_UNIQUE'\n",
    "\n",
    "# Use the X_test index to select the latest data from the full feature set\n",
    "df_temp = df_features.reset_index().copy()\n",
    "\n",
    "# Filter for the data corresponding to the test set indices (i.e., the last 30 days)\n",
    "test_indices = X_test.index\n",
    "df_latest = df_temp[df_temp.set_index(['Date', 'Ticker']).index.isin(test_indices)].copy()\n",
    "df_latest.set_index(['Date', 'Ticker'], inplace=True)\n",
    "df_latest = df_latest.loc[test_indices] # Align order with P_total\n",
    "\n",
    "# Get only the *very last* day of the test set for final recommendations\n",
    "latest_date = df_latest.index.get_level_values('Date').max()\n",
    "df_final_day = df_latest.loc[latest_date].reset_index()\n",
    "\n",
    "# Calculate the blending weight (Inverse Volatility)\n",
    "# We use the volatility column that was successfully added in Cell 11\n",
    "df_final_day['Volatility_Feature'] = df_final_day[VOLATILITY_COLUMN_NAME]\n",
    "df_final_day['Volatility_Feature'] = df_final_day['Volatility_Feature'].replace(0, 1e-6) # Prevent division by zero\n",
    "\n",
    "# Inverse Volatility is the core of the weighting strategy\n",
    "df_final_day['Blending_Weight'] = 1 / df_final_day['Volatility_Feature']\n",
    "denominator_sum = df_final_day['Blending_Weight'].sum()\n",
    "\n",
    "print(\"--- Generating Custom Blended Trade Recommendations (Volatility-Weighted Exposure) ---\")\n",
    "print(f\"SUCCESS: Volatility feature found. Total Blending Weight Sum: {denominator_sum:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Extract Final Probabilities\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# P_total has predictions for ALL test days. We only need the last day's predictions.\n",
    "TICKER_LIST = df_final_day['Ticker'].unique() \n",
    "TICKER_LIST.sort()\n",
    "\n",
    "# P_total_final_day is the last N predictions, where N is the number of tickers\n",
    "P_total_final_day = P_total[-len(TICKER_LIST):] \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Decision Making and Exposure Calculation\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "df_report = pd.DataFrame({'Ticker': TICKER_LIST, 'P_total': P_total_final_day})\n",
    "df_report = df_report.merge(\n",
    "    df_final_day[['Ticker', 'Blending_Weight', VOLATILITY_COLUMN_NAME]],\n",
    "    on='Ticker',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define trading thresholds\n",
    "BUY_THRESHOLD = 0.65\n",
    "SELL_THRESHOLD = 0.35\n",
    "df_report['Recommendation'] = np.select(\n",
    "    [df_report['P_total'] >= BUY_THRESHOLD, df_report['P_total'] <= SELL_THRESHOLD],\n",
    "    ['BUY (Long Position)', 'SELL (Short Position)'],\n",
    "    default='HOLD (Zero Exposure)'\n",
    ")\n",
    "\n",
    "# Calculate Volatility-Weighted Allocation\n",
    "df_report['Net Exposure (%)'] = 0.0 \n",
    "df_actionable = df_report[df_report['Recommendation'].isin(['BUY (Long Position)', 'SELL (Short Position)'])].copy()\n",
    "\n",
    "if not df_actionable.empty:\n",
    "    actionable_weight_sum = df_actionable['Blending_Weight'].sum()\n",
    "    \n",
    "    # Calculate normalized, volatility-weighted exposure\n",
    "    # Exposure = (Individual Inverse Volatility / Sum of Actionable Inverse Volatility) * 100\n",
    "    df_actionable['Net Exposure (%)'] = (df_actionable['Blending_Weight'] / actionable_weight_sum) * 100\n",
    "\n",
    "    # Apply sign for Long (Buy, positive) and Short (Sell, negative) positions\n",
    "    df_actionable['Net Exposure (%)'] = np.where(\n",
    "        df_actionable['Recommendation'] == 'SELL (Short Position)',\n",
    "        df_actionable['Net Exposure (%)'] * -1,\n",
    "        df_actionable['Net Exposure (%)']\n",
    "    )\n",
    "\n",
    "    # Update the main report DataFrame\n",
    "    df_report.set_index('Ticker', inplace=True)\n",
    "    df_actionable.set_index('Ticker', inplace=True)\n",
    "    df_report.update(df_actionable['Net Exposure (%)'])\n",
    "    df_report.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Final Report Generation\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "df_final_sorted = df_report.copy()\n",
    "df_final_sorted['Net Exposure (%)'] = df_final_sorted['Net Exposure (%)'].round(2).astype(str) + '%'\n",
    "\n",
    "df_final_sorted = df_final_sorted.sort_values(by='P_total', ascending=False)\n",
    "df_final_sorted = df_final_sorted[['Ticker', 'P_total', 'Recommendation', 'Net Exposure (%)']].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------\")\n",
    "print(\"| FINAL TRADING RECOMMENDATIONS FOR NEXT DAY ({0} Data) |\".format(latest_date.strftime('%Y-%m-%d')))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(df_final_sorted.to_string(index=False, float_format='%.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "141206b3-7ccb-4b70-a752-daf01bef7766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "| CORE FEATURE VECTORS (2025-12-01) - TABLE 1 of 2 |\n",
      "----------------------------------------------------------------------\n",
      "       Ticker  VOLATILITY (14d STD)     RSI  Polarity Score  Sentiment_Sample_Size  Sentiment_Score_Std\n",
      "  AXISBANK.NS                0.0043 62.2252         -0.9578                      1               0.0000\n",
      "BHARTIARTL.NS                0.0040 50.1061         -0.9475                      1               0.0000\n",
      "  HDFCBANK.NS                0.0040 54.7786          0.9077                      1               0.0000\n",
      " ICICIBANK.NS                0.0038 56.7217         -0.9621                      1               0.0000\n",
      "      INFY.NS                0.0037 61.3609          0.9029                      1               0.0000\n",
      "       ITC.NS                0.0034 45.7373         -0.9581                      1               0.0000\n",
      " KOTAKBANK.NS                0.0046 61.3578         -0.9628                      1               0.0000\n",
      "        LT.NS                0.0046 66.1390         -0.9630                      1               0.0000\n",
      "  RELIANCE.NS                0.0044 72.1896         -0.9448                      1               0.0000\n",
      "       TCS.NS                0.0045 57.2891         -0.9426                      1               0.0000\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| HYBRID & DERIVED FEATURE VECTORS (2025-12-01) - TABLE 2 of 2 |\n",
      "----------------------------------------------------------------------\n",
      "       Ticker  Sentiment-Volume  Sentiment-Volatility  Regulatory_Score\n",
      "  AXISBANK.NS           -0.9578             -223.5611            0.5000\n",
      "BHARTIARTL.NS           -0.9475             -239.7134            0.5000\n",
      "  HDFCBANK.NS            0.9077              226.4232            0.5000\n",
      " ICICIBANK.NS           -0.9621             -256.2985            0.5000\n",
      "      INFY.NS            0.9029              240.9203            0.5000\n",
      "       ITC.NS           -0.9581             -278.6063            0.5000\n",
      " KOTAKBANK.NS           -0.9628             -207.2297            0.5000\n",
      "        LT.NS           -0.9630             -209.5638            0.5000\n",
      "  RELIANCE.NS           -0.9448             -212.5654            0.5000\n",
      "       TCS.NS           -0.9426             -210.9535            0.5000\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Feature Vector Report for a Specific Prediction Date (SPLIT TABLES)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>  USER INPUT HERE  <<<<<<<<<<<<<<<<<<<\n",
    "# Reuse the date from the previous run\n",
    "PREDICTION_DATE_STR = '2025-12-01' \n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "PREDICTION_DATE = pd.to_datetime(PREDICTION_DATE_STR)\n",
    "VOLATILITY_COLUMN_NAME = '14d_STD_VOLATILITY_UNIQUE'\n",
    "\n",
    "# --- Data Preparation ---\n",
    "if PREDICTION_DATE not in X_test.index.get_level_values('Date'):\n",
    "    raise ValueError(f\"Date {PREDICTION_DATE_STR} not found in the test set.\")\n",
    "\n",
    "df_final_features = X_test.loc[PREDICTION_DATE].reset_index()\n",
    "\n",
    "df_final_features.rename(\n",
    "    columns={\n",
    "        VOLATILITY_COLUMN_NAME: 'VOLATILITY (14d STD)',\n",
    "        'FinBERT_Polarity_Score': 'Polarity Score',\n",
    "        'Sentiment_Volume_Hybrid': 'Sentiment-Volume',\n",
    "        'Sentiment_Volatility_Hybrid': 'Sentiment-Volatility'\n",
    "    }, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_report = df_final_features.copy()\n",
    "df_report = df_report.sort_values(by='Ticker').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Table 1: Core Features ---\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(f\"| CORE FEATURE VECTORS ({PREDICTION_DATE.strftime('%Y-%m-%d')}) - TABLE 1 of 2 |\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "core_columns = [\n",
    "    'Ticker', \n",
    "    'VOLATILITY (14d STD)',\n",
    "    'RSI',\n",
    "    'Polarity Score',\n",
    "    'Sentiment_Sample_Size',\n",
    "    'Sentiment_Score_Std'\n",
    "]\n",
    "print(df_report[core_columns].to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "\n",
    "# --- Table 2: Hybrid and Derived Features ---\n",
    "print(\"\\n----------------------------------------------------------------------\")\n",
    "print(f\"| HYBRID & DERIVED FEATURE VECTORS ({PREDICTION_DATE.strftime('%Y-%m-%d')}) - TABLE 2 of 2 |\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "hybrid_columns = [\n",
    "    'Ticker', \n",
    "    'Sentiment-Volume',\n",
    "    'Sentiment-Volatility',\n",
    "    'Regulatory_Score'\n",
    "]\n",
    "print(df_report[hybrid_columns].to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b52eb28-f86e-4e4f-8591-dd8edf25eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Liquidity (Slippage) Check based on 60-Day ADV ---\n",
      "Total Portfolio Capital: Rs 5,000,000\n",
      "Max Single Stock Allocation (MA): Rs 1,000,000\n",
      "Liquidity Threshold: 15% of 60-Day ADV\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| FINAL TRADING DECISIONS AFTER LIQUIDITY CHECK (2025-12-11) |\n",
      "----------------------------------------------------------------------\n",
      "Total Actionable Capital Allocated: Rs 3,613,000\n",
      "Total Unspent Capital: Rs 1,387,000\n",
      "----------------------------------------------------------------------\n",
      "       Ticker  P_total        Recommendation Capital Allocation (Rs)  60d ADV (Shares)\n",
      "  HDFCBANK.NS 0.993949   BUY (Long Position)              Rs 598,500 19,613,195 shares\n",
      " ICICIBANK.NS 0.931053   BUY (Long Position)              Rs 619,500 12,027,178 shares\n",
      "BHARTIARTL.NS 0.906120   BUY (Long Position)              Rs 717,500  7,398,788 shares\n",
      "       ITC.NS 0.778665   BUY (Long Position)              Rs 616,000 12,436,238 shares\n",
      "        LT.NS 0.675089        ILLIQUID STOCK                    Rs 0          0 shares\n",
      "  AXISBANK.NS 0.597268        ILLIQUID STOCK                    Rs 0          0 shares\n",
      "  RELIANCE.NS 0.292875 SELL (Short Position)              Rs 490,000 10,705,600 shares\n",
      "       TCS.NS 0.094451        ILLIQUID STOCK                    Rs 0          0 shares\n",
      " KOTAKBANK.NS 0.084240        ILLIQUID STOCK                    Rs 0          0 shares\n",
      "      INFY.NS 0.008840 SELL (Short Position)              Rs 571,500  8,416,822 shares\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Liquidity (Slippage) Check based on 60-Day ADV (Updated Constraints)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Define NEW Financial Constraints\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "TOTAL_CAPITAL = 5_000_000      # Rs 50,00,000 (Updated from 1 Cr)\n",
    "MAX_SINGLE_STOCK_ALLOCATION = 1_000_000 # Rs 10,00,000 (Updated from 20 Lakhs)\n",
    "LIQUIDITY_CHECK_PERCENTAGE = 0.15 # 15% of 60-day ADV\n",
    "\n",
    "print(\"--- Liquidity (Slippage) Check based on 60-Day ADV ---\")\n",
    "print(f\"Total Portfolio Capital: Rs {TOTAL_CAPITAL:,.0f}\")\n",
    "print(f\"Max Single Stock Allocation (MA): Rs {MAX_SINGLE_STOCK_ALLOCATION:,.0f}\")\n",
    "print(f\"Liquidity Threshold: 15% of 60-Day ADV\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Calculate 60-Day Average Daily Volume (ADV) and Prepare Data\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# We need the full df_features to calculate the 60-day rolling average of 'Volume'\n",
    "df_adv = df_features.reset_index().copy()\n",
    "\n",
    "# Note: Assuming 'Volume' column exists in df_features\n",
    "if 'Volume' not in df_adv.columns:\n",
    "    raise KeyError(\"The 'Volume' column is required in df_features to calculate ADV.\")\n",
    "\n",
    "# Calculate 60-day rolling ADV (Average Daily Volume)\n",
    "df_adv['60d_ADV'] = df_adv.groupby('Ticker')['Volume'].transform(\n",
    "    lambda x: x.rolling(window=60, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Filter ADV to the latest date used in the prediction (from Cell 15)\n",
    "df_final_day_adv = df_adv[df_adv['Date'] == latest_date].copy()\n",
    "df_final_day_adv = df_final_day_adv[['Ticker', '60d_ADV']]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Merge ADV and Apply Liquidity Filter\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "df_liquidity_check = df_final_sorted.copy()\n",
    "\n",
    "# Convert 'Net Exposure (%)' back to a float for calculation (retaining the sign for now)\n",
    "df_liquidity_check['Net Exposure (%)_float'] = (\n",
    "    df_liquidity_check['Net Exposure (%)'].str.replace('%', '', regex=False).astype(float)\n",
    ")\n",
    "\n",
    "# Merge ADV data\n",
    "df_liquidity_check = df_liquidity_check.merge(\n",
    "    df_final_day_adv, on='Ticker', how='left'\n",
    ")\n",
    "\n",
    "# Calculate the initial absolute allocation before any checks\n",
    "df_liquidity_check['Absolute Allocation (Rs)_Initial'] = (\n",
    "    df_liquidity_check['Net Exposure (%)_float'].abs() / 100\n",
    ") * TOTAL_CAPITAL\n",
    "\n",
    "# --- Apply the Illiquidity Check ---\n",
    "# Liquidity Check Rule: If MAX_SINGLE_STOCK_ALLOCATION > (0.15 * 60d_ADV), mark as illiquid\n",
    "liquidity_threshold_abs = df_liquidity_check['60d_ADV'] * LIQUIDITY_CHECK_PERCENTAGE\n",
    "\n",
    "illiquid_mask = (MAX_SINGLE_STOCK_ALLOCATION > liquidity_threshold_abs)\n",
    "\n",
    "# If illiquid, set allocation to zero and update recommendation\n",
    "df_liquidity_check.loc[illiquid_mask, 'Recommendation'] = 'ILLIQUID STOCK'\n",
    "df_liquidity_check.loc[illiquid_mask, 'Absolute Allocation (Rs)_Final'] = 0.0\n",
    "df_liquidity_check.loc[illiquid_mask, '60d_ADV'] = 0 # Zero out ADV for clean reporting\n",
    "\n",
    "# For liquid stocks, the final allocation is the initial calculation (using absolute value)\n",
    "df_liquidity_check.loc[~illiquid_mask, 'Absolute Allocation (Rs)_Final'] = df_liquidity_check['Absolute Allocation (Rs)_Initial']\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Final Recalculation and Report Generation\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "total_spent_capital = df_liquidity_check['Absolute Allocation (Rs)_Final'].sum()\n",
    "\n",
    "# We still calculate Net Exposure internally to correctly calculate total_spent_capital,\n",
    "# but we will NOT include it in the final report DataFrame.\n",
    "df_liquidity_check['Net Exposure (%)'] = (\n",
    "    df_liquidity_check['Absolute Allocation (Rs)_Final'] / TOTAL_CAPITAL\n",
    ") * 100\n",
    "\n",
    "# Formatting\n",
    "# Capital Allocation (Rs) is always shown in absolute terms as requested\n",
    "df_liquidity_check['Capital Allocation (Rs)_formatted'] = (\n",
    "    df_liquidity_check['Absolute Allocation (Rs)_Final'].apply(lambda x: f\"Rs {x:,.0f}\")\n",
    ")\n",
    "df_liquidity_check['60d_ADV_formatted'] = (\n",
    "    df_liquidity_check['60d_ADV'].apply(lambda x: f\"{x:,.0f} shares\")\n",
    ")\n",
    "\n",
    "\n",
    "df_report_final = df_liquidity_check[[\n",
    "    'Ticker', \n",
    "    'P_total', \n",
    "    'Recommendation', \n",
    "    'Capital Allocation (Rs)_formatted',\n",
    "    '60d_ADV_formatted' # <--- NET EXPOSURE REMOVED HERE\n",
    "]].rename(columns={\n",
    "    'Capital Allocation (Rs)_formatted': 'Capital Allocation (Rs)',\n",
    "    '60d_ADV_formatted': '60d ADV (Shares)'\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------\")\n",
    "print(f\"| FINAL TRADING DECISIONS AFTER LIQUIDITY CHECK ({latest_date.strftime('%Y-%m-%d')}) |\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(f\"Total Actionable Capital Allocated: Rs {total_spent_capital:,.0f}\")\n",
    "print(f\"Total Unspent Capital: Rs {TOTAL_CAPITAL - total_spent_capital:,.0f}\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "print(df_report_final.to_string(index=False, float_format='%.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d559e-710c-411d-809e-7d8c6e580e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
